"""
More detail, please see:
https://huggingface.co/augmxnt/shisa-7b-v1
"""
import copy
from dataclasses import asdict

import torch
from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from config import GLOBAL_CONFIG as G_CFG
from llm.pipeline import LLMPipeline, LLMQuery, LLMResponse, Role, Chat
from loguru import logger
from common.datacls import ModelNameConst as MNC

# Global attributes
_app = Flask(__name__)  # Flask application instance

_host: str  # Host address for the Flask application
_port: int  # Port number for the Flask application
_debug: bool  # Debug mode flag for the Flask application

_tokenizer: any  # Tokenizer for the language _model
_model: any  # Language _model for generating responses
_streamer: TextStreamer  # Text streamer for streaming responses


def init():
    """
    Initializes the application with the given configuration.
    """
    logger.info(f'💭 Application {MNC.SHISA} is initializing...')
    global _tokenizer, _model, _streamer, _debug, _host, _port

    llm_cfg = G_CFG.large_language_model
    _host, _port, _debug = llm_cfg.host, llm_cfg.port, llm_cfg.debug
    model_path = llm_cfg.models[0].model_path

    logger.info(f'💭 Model {MNC.SHISA} is loading...')
    _tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
    _model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
    ).to('cuda')
    _streamer = TextStreamer(_tokenizer, skip_prompt=True)
    assert _tokenizer and _model and _streamer, f'❌️ Model {MNC.SHISA} failed to load.'
    logger.info(f'💭 Model {MNC.SHISA} loaded successfully.')

    logger.info(f'💭 Application {MNC.CHATGLM3} initialized successfully.')


def start():
    """
    Starts the Flask application with the configured host, port, and debug mode.
    """
    logger.info(f'💭 Application {MNC.SHISA} is starting...')
    _app.run(host=_host, port=_port, debug=_debug)
    logger.info(f'💭 Application {MNC.SHISA} is stopped.')


def _predict(llm_query: LLMQuery):
    """
    Generates a response from the language _model based on the given query.

    Args:
        llm_query (llm.pipeline.LLMQuery): The query containing the text and conversation history.

    Returns:
        llm.pipeline.LLMResponse: The response generated by the language _model, including the response text and updated conversation history.
    """
    assert len(llm_query.history) > 0 and llm_query.history[0].role == Role.SYSTEM, f'Must includes system prompt'

    llm_query_history = copy.deepcopy(llm_query.history)
    llm_query_history.append(Chat(role=Role.USER, content=llm_query.text))
    history = [{"role": chat.role, "content": chat.content} for chat in llm_query_history]

    inputs = _tokenizer.apply_chat_template(history, add_generation_prompt=True, return_tensors="pt")
    first_param_device = next(_model.parameters()).device
    inputs = inputs.to(first_param_device)

    with torch.no_grad():
        outputs = _model.generate(
            inputs,
            pad_token_id=_tokenizer.eos_token_id,
            max_new_tokens=500,
            temperature=0.5,
            repetition_penalty=1.15,
            top_p=0.95,
            do_sample=True,
            streamer=_streamer,
        )

    new_tokens = outputs[0, inputs.size(1):]
    response = _tokenizer.decode(new_tokens, skip_special_tokens=True)
    llm_query.history.append(Chat(role=Role.ASSISTANT, content=response))
    return LLMResponse(response=response, history=llm_query.history)


@_app.route('/llm/predict', methods=['POST', 'GET'])
def _handle_predict():
    """
    Handles prediction requests by generating a response from the language _model based on the received query.

    Returns:
        Response: A JSON response containing the generated response and conversation history.
    """
    json_val = request.get_json()
    llm_query = LLMPipeline.parse_query_from_json(json_val)
    llm_response = _predict(llm_query)
    return jsonify(asdict(llm_response))


@_app.route('/llm/stream-predict', methods=['GET', 'POST'])
def _handle_predict():
    """
    Handles streaming prediction requests. This route has not been implemented yet.

    Raises:
        NotImplementedError: Indicates that the route is not implemented.
    """
    raise NotImplementedError('This route has not been implemented yet.')
