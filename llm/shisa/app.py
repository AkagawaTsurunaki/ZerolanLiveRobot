"""
More detail, please see:
https://huggingface.co/augmxnt/shisa-7b-v1
"""
import copy
from dataclasses import asdict

import torch
from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

from common.datacls import LLMQuery, LLMResponse, Chat, Role
from config import GlobalConfig
from llm.pipeline import LLMPipeline

# Global attributes
_app = Flask(__name__)  # Flask application instance

_host: str  # Host address for the Flask application
_port: int  # Port number for the Flask application
_debug: bool  # Debug mode flag for the Flask application

_tokenizer: any  # Tokenizer for the language _model
_model: any  # Language _model for generating responses
_streamer: TextStreamer  # Text streamer for streaming responses


def init(cfg: GlobalConfig):
    """
    Initializes the application with the given configuration.

    Args:
        cfg (GlobalConfig): Configuration object containing settings for the application.
    """
    global _tokenizer, _model, _streamer, _debug, _host, _port

    _host = cfg.large_language_model.host
    _port = cfg.large_language_model.port
    _debug = cfg.large_language_model.debug

    model_path = cfg.large_language_model.models[0].model_path
    _tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
    _model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
    ).to('cuda')
    _streamer = TextStreamer(_tokenizer, skip_prompt=True)


def start():
    """
    Starts the Flask application with the configured host, port, and debug mode.
    """
    _app.run(host=_host, port=_port, debug=_debug)


def _predict(llm_query: LLMQuery):
    """
    Generates a response from the language _model based on the given query.

    Args:
        llm_query (LLMQuery): The query containing the text and conversation history.

    Returns:
        LLMResponse: The response generated by the language _model, including the response text and updated conversation history.
    """
    assert len(llm_query.history) > 0 and llm_query.history[0].role == Role.SYSTEM, f'Must includes system prompt'

    llm_query_history = copy.deepcopy(llm_query.history)
    llm_query_history.append(Chat(role=Role.USER, content=llm_query.text))
    history = [{"role": chat.role, "content": chat.content} for chat in llm_query_history]

    inputs = _tokenizer.apply_chat_template(history, add_generation_prompt=True, return_tensors="pt")
    first_param_device = next(_model.parameters()).device
    inputs = inputs.to(first_param_device)

    with torch.no_grad():
        outputs = _model.generate(
            inputs,
            pad_token_id=_tokenizer.eos_token_id,
            max_new_tokens=500,
            temperature=0.5,
            repetition_penalty=1.15,
            top_p=0.95,
            do_sample=True,
            streamer=_streamer,
        )

    new_tokens = outputs[0, inputs.size(1):]
    response = _tokenizer.decode(new_tokens, skip_special_tokens=True)
    llm_query.history.append(Chat(role=Role.ASSISTANT, content=response))
    return LLMResponse(response=response, history=llm_query.history)


@_app.route('/llm/predict', methods=['POST', 'GET'])
def _handle_predict():
    """
    Handles prediction requests by generating a response from the language _model based on the received query.

    Returns:
        Response: A JSON response containing the generated response and conversation history.
    """
    json_val = request.get_json()
    llm_query = LLMPipeline.parse_query_from_json(json_val)
    llm_response = _predict(llm_query)
    return jsonify(asdict(llm_response))


@_app.route('/llm/stream-predict', methods=['GET', 'POST'])
def _handle_predict():
    """
    Handles streaming prediction requests. This route has not been implemented yet.

    Raises:
        NotImplementedError: Indicates that the route is not implemented.
    """
    raise NotImplementedError('This route has not been implemented yet.')
