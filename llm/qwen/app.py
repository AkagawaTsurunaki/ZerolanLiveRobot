from dataclasses import asdict

from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer

from common.datacls import LLMResponse, Chat, LLMQuery
from llm.pipeline import LLMPipeline

# Global attributes
app = Flask(__name__)  # Flask application instance
_tokenizer: any  # Tokenizer for the language model
_model: any  # Language model for generating responses


def _predict(llm_query: LLMQuery):
    """
    Generates a response from the language model based on the given query.

    Args:
        llm_query (LLMQuery): The query containing the text and conversation history.

    Returns:
        LLMResponse: The response generated by the language model, including the response text and updated conversation history.
    """
    query = llm_query.text
    history = llm_query.history
    history = [{'role': chat.role, 'content': chat.content} for chat in history]
    response, history = _model.chat(_tokenizer, query, history=history)
    history = [Chat(role=chat['role'], content=chat['content']) for chat in history]
    llm_response = LLMResponse(response=response, history=history)
    return llm_response


@app.route('/llm/predict', methods=['GET', 'POST'])
def _handle_predict():
    """
    Handles prediction requests by generating a response from the language model based on the received query.

    Returns:
        Response: A JSON response containing the generated response and conversation history.
    """
    json_val = request.get_json()
    llm_query = LLMPipeline.parse_query_from_json(json_val)
    llm_response = _predict(llm_query)
    return jsonify(asdict(llm_response))


@app.route('/llm/stream-predict', methods=['GET', 'POST'])
def _handle_stream_predict():
    """
    Handles streaming prediction requests. This route has not been implemented yet.

    Raises:
        NotImplementedError: Indicates that the route is not implemented.
    """
    raise NotImplementedError('This route has not been implemented yet.')


def start(model_path, mode, host, port, debug):
    """
    Initializes and starts the Flask application with the specified model, mode, host, port, and debug settings.

    Args:
        model_path (str): Path to the pretrained model.
        mode (str): Mode for loading the model (bf16, fp16, cpu).
        host (str): Host address for the Flask application.
        port (int): Port number for the Flask application.
        debug (bool): Debug mode flag for the Flask application.
    """
    global _model, _tokenizer
    # Load model on given mode
    if mode == 'bf16':
        _model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True,
                                                      bf16=True).eval()
    elif mode == 'fp16':
        _model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True,
                                                      fp16=True).eval()
    elif mode == 'cpu':
        _model = AutoModelForCausalLM.from_pretrained(model_path, device_map="cpu", trust_remote_code=True).eval()
    else:
        _model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True).eval()

    # Load auto _tokenizer
    _tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    # Run application
    app.run(host=host, port=port, debug=debug)
