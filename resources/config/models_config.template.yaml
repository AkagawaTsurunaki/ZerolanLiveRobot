ASR:
  SpeechParaformerModelConfig:
    # https://www.modelscope.cn/models/iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1
    model_path: iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1
    chunk_size: [ 0, 10, 5 ] # [0, 10, 5] 600ms, [0, 8, 4] 480ms
    encoder_chunk_look_back: 4 # number of chunks to lookback for encoder self-attention
    decoder_chunk_look_back: 1 # number of encoder chunks to lookback for decoder cross-attention
    version: v2.0.4
    chunk_stride: 9600

LLM:
  ChatGLM3ModelConfig:
    # https://github.com/THUDM/ChatGLM3
    model_path: THUDM/chatglm3-6b
    quantize: null # null 将不会启用模型量化；4 为 4-Bit 量化；8 为 8-Bit 量化
    device: cuda

  QwenModelConfig:
    # https://huggingface.co/Qwen/Qwen-7B-Chat
    model_path: Qwen/Qwen-7B-Chat
    quantize: 4 # null 将不会启用模型量化；4 为 4-Bit 量化；8 为 8-Bit 量化
    device: cuda
    precise: bf16 # bf16，fp16，或全精度

  ShisaModelConfig:
    # https://huggingface.co/augmxnt/shisa-7b-v1
    model_path: augmxnt/shisa-7b-v1
    device: cuda

  YiModelConfig:
    # https://www.modelscope.cn/models/01ai/Yi-6B-Chat
    model_path: 01-ai/Yi-6B-Chat
    device: auto

IC:
  BlipModelConfig:
    # https://huggingface.co/Salesforce/blip-image-captioning-large
    model_path: Salesforce/blip-image-captioning-large
    device: cuda

OCR:
  PaddleOCRModelConfig:
    # https://gitee.com/paddlepaddle/PaddleOCR
    model_path: paddlepaddle/PaddleOCR

VidCap:
  HiteaBaseModelConfig:
    # https://www.modelscope.cn/models/iic/multi-modal_hitea_video-captioning_base_en
    model_path: damo/multi-modal_hitea_video-captioning_base_en
